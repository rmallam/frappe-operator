name: E2E Test (Kind)

# Run E2E after CI completes (reuse CI-built image). Manual run via workflow_dispatch builds image.
on:
  workflow_run:
    workflows: [CI]
    types: [completed]
    branches: [main, master]
  workflow_dispatch:

jobs:
  build-operator:
    name: Build Operator Image (once)
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    permissions:
      contents: read
      packages: write
    outputs:
      image: ${{ steps.meta.outputs.image }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push operator image
        id: meta
        run: |
          IMAGE="ghcr.io/${{ github.repository }}:e2e-${{ github.run_id }}"
          docker build -t "$IMAGE" .
          docker push "$IMAGE"
          echo "image=$IMAGE" >> $GITHUB_OUTPUT

  e2e-test:
    name: End-to-End Test on Kind (${{ matrix.platform }})
    runs-on: ubuntu-latest
    needs: [build-operator]
    # Run when: (triggered by CI push and CI succeeded) OR (manual run and build-operator succeeded).
    # When triggered by workflow_run, build-operator is skipped; use always() so this job still runs.
    if: ${{ always() && !cancelled() && ((github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success' && github.event.workflow_run.event == 'push') || (github.event_name == 'workflow_dispatch' && needs.build-operator.result == 'success')) }}
    permissions:
      contents: read
      attestations: write
      id-token: write
      packages: read

    strategy:
      matrix:
        platform: [k8s, openshift]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event_name == 'workflow_run' && github.event.workflow_run.head_sha || github.sha }}

      - name: Create Kind Cluster
        uses: helm/kind-action@v1.8.0
        with:
          cluster_name: frappe-e2e-${{ matrix.platform }}

      - name: Resolve and load operator image
        run: |
          # After CI: use same image (sha-XXXXXXX format from docker/metadata-action). Manual run: use image from build-operator.
          if [ -n "${{ needs.build-operator.outputs.image }}" ]; then
            IMAGE="${{ needs.build-operator.outputs.image }}"
          else
            # CI uses docker/metadata-action type=sha,format=short which produces "sha-XXXXXXX"
            SHORT_SHA=$(echo ${{ github.event.workflow_run.head_sha }} | cut -c1-7)
            IMAGE="ghcr.io/${{ github.repository }}:sha-${SHORT_SHA}"
          fi
          TAG="${IMAGE##*:}"
          docker login ghcr.io -u ${{ github.actor }} -p ${{ secrets.GITHUB_TOKEN }}
          docker pull "$IMAGE"
          kind load docker-image "$IMAGE" --name frappe-e2e-${{ matrix.platform }}
          echo "OPERATOR_IMAGE=$IMAGE" >> $GITHUB_ENV
          echo "OPERATOR_IMAGE_TAG=$TAG" >> $GITHUB_ENV

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Add Helm Repositories
        run: |
          helm repo add mariadb-operator https://mariadb-operator.github.io/mariadb-operator
          helm repo add keda https://kedacore.github.io/charts
          helm repo update

      - name: Install MariaDB Operator
        run: |
          helm upgrade --install mariadb-operator mariadb-operator/mariadb-operator \
            --namespace frappe-operator-system \
            --create-namespace \
            --set crds.enabled=true \
            --wait --timeout 10m

      - name: Install KEDA
        run: |
          helm upgrade --install keda keda/keda \
            --namespace frappe-operator-system \
            --create-namespace \
            --set crds.install=true \
            --wait --timeout 10m

      - name: Mock OpenShift Environment
        if: matrix.platform == 'openshift'
        run: |
          echo "Installing OpenShift Route CRD to mock platform..."
          kubectl apply -f https://raw.githubusercontent.com/openshift/router/main/deploy/route_crd.yaml
          sleep 5

      - name: Install Frappe Operator
        run: |
          # Use image from CI (short SHA) or from build-operator (manual run)
          REPO="ghcr.io/${{ github.repository }}"
          helm upgrade --install frappe-operator ./helm/frappe-operator \
            --set operator.image.repository="$REPO" \
            --set operator.image.tag="${OPERATOR_IMAGE_TAG}" \
            --set operator.image.pullPolicy=Never \
            --set mariadb.enabled=true \
            --namespace frappe-operator-system \
            --create-namespace \
            --wait \
            --timeout 10m

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.22'
          cache: true

      - name: Run Integration Tests
        env:
          INTEGRATION_TEST: "true"
          # KUBECONFIG is set by helm/kind-action so go test uses the Kind cluster
        run: |
          go test -v -count=1 ./test/integration/...


      - name: Deploy Test Bench and Site
        run: |
          kubectl apply -f examples/kind-e2e-manifests.yaml

      - name: Verify Platform Detection
        run: |
          echo "Waiting for operator deployment to be available..."
          kubectl wait --for=condition=available deployment/frappe-operator-controller-manager -n frappe-operator-system --timeout=120s || true
          echo "Checking operator logs for platform detection..."
          # Retry log check for up to 3 minutes; use deployment name and accept manager/leader-election log lines
          for i in $(seq 1 18); do
            LOGS=$(kubectl logs -n frappe-operator-system deployment/frappe-operator-controller-manager --tail=1000 2>/dev/null || echo "")
            
            if [ "${{ matrix.platform }}" == "openshift" ]; then
              if echo "$LOGS" | grep -i "OpenShift platform detected"; then
                echo "SUCCESS: OpenShift platform detected correctly."
                exit 0
              fi
            else
              # Vanilla K8s: confirm no OpenShift and manager has started (our log or controller-runtime leader election)
              if ! echo "$LOGS" | grep -i "OpenShift platform detected"; then
                if echo "$LOGS" | grep -qiE "starting manager|Leader elected|attempting to acquire leader"; then
                  echo "SUCCESS: Vanilla Kubernetes environment confirmed (no OpenShift detected)."
                  exit 0
                fi
              fi
            fi
            echo "Waiting for operator to start and detect platform (attempt $i/18)..."
            sleep 10
          done
          echo "FAILED: Platform detection verification timed out."
          echo "--- Operator pods ---"
          kubectl get pods -n frappe-operator-system -l control-plane=controller-manager -o wide 2>/dev/null || true
          echo "--- Last 80 lines of operator log ---"
          kubectl logs -n frappe-operator-system deployment/frappe-operator-controller-manager --tail=80 2>/dev/null || true
          exit 1

      - name: Wait for FrappeBench to be Ready
        run: |
          echo "Waiting for FrappeBench to reach Ready phase..."
          for i in {1..60}; do
            PHASE=$(kubectl get frappebench e2e-test-bench -n default -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
            echo "Current Phase: $PHASE"
            if [ "$PHASE" == "Ready" ]; then
              echo "FrappeBench is Ready!"
              exit 0
            fi
            sleep 10
          done
          echo "ERROR: FrappeBench did not reach Ready phase within timeout"
          exit 1

      - name: Apply Advanced Pod Configuration
        run: |
          # podConfig with e2e-custom-label is set in kind-e2e-manifests.yaml so labels exist at create time.
          # Optional: patch to exercise in-place updates (operator would need to sync podConfig to existing deployments).
          echo "Advanced pod config (e2e-custom-label) is in bench spec from manifest; no patch needed for verify."

      - name: Verify Advanced Pod Configuration
        run: |
          echo "Verifying custom labels are propagated to pods..."
          # We check if *any* pod has the label. Note: Rolling updates might take time, so we poll.
          for i in {1..30}; do
            PODS_WITH_LABEL=$(kubectl get pods -l e2e-custom-label=verified -n default -o name)
            if [ ! -z "$PODS_WITH_LABEL" ]; then
              echo "SUCCESS: Custom label 'e2e-custom-label=verified' found on pods:"
              echo "$PODS_WITH_LABEL"
              exit 0
            fi
            echo "Waiting for pods to have custom label... ($i/30)"
            sleep 5
          done
          echo "ERROR: Custom label not propagated to pods within timeout."
          exit 1

      - name: Wait for FrappeSite to be Ready (with ERPNext install)
        run: |
          # OpenShift/Kind can be slower; allow 20 min for openshift, 10 min for k8s
          MAX_ATTEMPTS=60
          if [ "${{ matrix.platform }}" == "openshift" ]; then MAX_ATTEMPTS=120; fi
          echo "Waiting for FrappeSite to reach Ready phase (site install includes ERPNext, max $MAX_ATTEMPTS attempts)..."
          for i in $(seq 1 $MAX_ATTEMPTS); do
            PHASE=$(kubectl get frappesite e2e-test-site -n default -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
            echo "Current Phase: $PHASE (attempt $i/$MAX_ATTEMPTS)"
            if [ "$PHASE" == "Ready" ]; then
              echo "FrappeSite is Ready!"
              exit 0
            fi
            sleep 10
          done
          echo "ERROR: FrappeSite did not reach Ready phase within timeout"
          exit 1

      - name: Verify ERPNext installed on site (bench list-apps)
        run: |
          # Site is Ready after init job succeeds; list-apps can lag or exec can be briefly unavailable. Poll with retries.
          echo "Verifying ERPNext via bench --site e2e.localhost list-apps (polling up to 2.5 min)..."
          POD_NAME=$(kubectl get pods -l app=frappe,bench=e2e-test-bench -n default -o jsonpath='{.items[0].metadata.name}')
          echo "Using pod: $POD_NAME"
          sleep 15
          for i in $(seq 1 15); do
            APPS=$(kubectl exec -n default $POD_NAME -- bash -c "cd /home/frappe/frappe-bench && bench --site e2e.localhost list-apps" 2>&1) || true
            echo "Attempt $i/15 - list-apps output: $APPS"
            if echo "$APPS" | grep -qi "erpnext"; then
              echo "SUCCESS: ERPNext is installed on the site (verified via bench list-apps)."
              exit 0
            fi
            [ $i -lt 15 ] && sleep 10
          done
          echo "ERROR: ERPNext not found in bench --site e2e.localhost list-apps output after 15 attempts"
          exit 1

      - name: Verify In-Cluster Accessibility
        run: |
          echo "Testing site accessibility from inside the cluster..."
          # Get the name of any running pod to use for the check
          POD_NAME=$(kubectl get pods -l app=frappe,bench=e2e-test-bench -o jsonpath='{.items[0].metadata.name}')
          echo "Using pod: $POD_NAME"
          
          # Curl the nginx service with the correct Host header
          RESPONSE=$(kubectl exec $POD_NAME -- curl -sI -H "Host: e2e.localhost" http://e2e-test-bench-nginx:8080)
          echo "Response:"
          echo "$RESPONSE"
          
          if echo "$RESPONSE" | grep -q "HTTP/1.1 200 OK"; then
            echo "SUCCESS: Site e2e.localhost is accessible from inside the cluster."
          else
            echo "ERROR: Site e2e.localhost is NOT accessible (unexpected response)."
            exit 1
          fi

      - name: Verify Resource Creation (Ingress/Route)
        run: |
          if [ "${{ matrix.platform }}" == "openshift" ]; then
            echo "Verifying OpenShift Route Creation..."
            if [ $(kubectl get routes.route.openshift.io -A --no-headers 2>/dev/null | wc -l) -eq 0 ]; then
              echo "FAILED: No OpenShift Routes found"
              exit 1
            fi
            echo "SUCCESS: OpenShift Route created."
          else
            echo "Verifying Ingress Creation..."
            if [ $(kubectl get ingress -A --no-headers 2>/dev/null | wc -l) -eq 0 ]; then
              echo "FAILED: No Ingresses found"
              exit 1
            fi
            echo "SUCCESS: Ingress created."
          fi

      - name: Verify Security Compliance (runAsNonRoot)
        run: |
          echo "Verifying security compliance (runAsNonRoot) on ${{ matrix.platform }}..."
          # Check all pods in default namespace (Bench, Workers, etc.)
          NON_COMPLIANT=$(kubectl get pods -n default -o jsonpath='{range .items[*]}{.metadata.name}{": runAsNonRoot="}{.spec.securityContext.runAsNonRoot}{"\n"}{end}' | grep "runAsNonRoot=false" || true)
          if [ ! -z "$NON_COMPLIANT" ]; then
            echo "FAILED: Some pods are not running as non-root:"
            echo "$NON_COMPLIANT"
            exit 1
          fi
          echo "SUCCESS: All pods are compliant with non-root security requirements."

      - name: Verify Prometheus Metrics Endpoint
        run: |
          echo "Verifying Prometheus metrics are exposed..."
          # Helm install exposes metrics on port 8080 (no auth proxy)
          METRICS_PORT=8080
          kubectl port-forward -n frappe-operator-system svc/frappe-operator-controller-manager-metrics-service ${METRICS_PORT}:${METRICS_PORT} &
          PF_PID=$!
          sleep 5
          
          METRICS=$(curl -s http://localhost:${METRICS_PORT}/metrics 2>/dev/null | head -50 || true)
          kill $PF_PID 2>/dev/null || true
          
          if echo "$METRICS" | grep -qE "(frappe_operator|controller_runtime|workqueue)"; then
            echo "SUCCESS: Prometheus metrics endpoint is working."
            echo "Sample metrics:"
            echo "$METRICS" | grep -E "^(frappe_operator|controller_runtime)" | head -10
          else
            echo "WARNING: Could not verify metrics. Checking operator logs for metrics setup..."
            kubectl logs -n frappe-operator-system -l control-plane=controller-manager --tail=100 | grep -i metric || true
          fi

      - name: Verify Job TTL Configuration
        run: |
          echo "Verifying Job TTL is configured on completed jobs..."
          # Get all jobs created by the operator
          JOBS=$(kubectl get jobs -n default -o json)
          
          # Check if TTLSecondsAfterFinished is set
          TTL_CHECK=$(echo "$JOBS" | jq -r '.items[] | select(.metadata.name | contains("site")) | "\(.metadata.name): TTL=\(.spec.ttlSecondsAfterFinished // "NOT_SET")"')
          echo "Job TTL status:"
          echo "$TTL_CHECK"
          
          # Verify at least one job has TTL set
          if echo "$TTL_CHECK" | grep -q "TTL=3600"; then
            echo "SUCCESS: Jobs have TTLSecondsAfterFinished configured (3600s)."
          else
            echo "WARNING: Could not verify Job TTL. Jobs may not have completed yet or TTL not set."
            kubectl get jobs -n default -o wide
          fi

      - name: Verify Resource Limits on Pods
        run: |
          echo "Verifying resource limits are set on Frappe pods..."
          # Check that pods have resource requests/limits
          PODS_RESOURCES=$(kubectl get pods -n default -l app=frappe -o jsonpath='{range .items[*]}{.metadata.name}: requests={.spec.containers[0].resources.requests}, limits={.spec.containers[0].resources.limits}{"\n"}{end}')
          echo "Pod resource configuration:"
          echo "$PODS_RESOURCES"
          
          # Verify at least one pod has memory limits
          if kubectl get pods -n default -l app=frappe -o json | jq -e '.items[].spec.containers[].resources.limits.memory' > /dev/null 2>&1; then
            echo "SUCCESS: Pods have resource limits configured."
          else
            echo "INFO: Pods may be using default resources from CRD or not yet created."
          fi

      - name: Verify Webhook Validation (Optional)
        continue-on-error: true
        run: |
          echo "Testing webhook validation with invalid CR..."
          # Create an invalid FrappeSite (missing required benchRef)
          cat <<EOF | kubectl apply -f - 2>&1 | tee /tmp/webhook-test.log || true
          apiVersion: vyogo.tech/v1alpha1
          kind: FrappeSite
          metadata:
            name: invalid-test-site
            namespace: default
          spec:
            siteName: "invalid.localhost"
            # Missing benchRef - should be rejected by webhook
          EOF
          
          if grep -qi "denied\|rejected\|invalid\|required" /tmp/webhook-test.log; then
            echo "SUCCESS: Webhook correctly rejected invalid CR."
          else
            echo "INFO: Webhook validation test inconclusive (webhooks may not be enabled in test cluster)."
          fi
          # Cleanup if it was accidentally created
          kubectl delete frappesite invalid-test-site -n default --ignore-not-found 2>/dev/null || true

      - name: Test attestation
        run: |
          echo '{ "workflow": "e2e-test", "platform": "${{ matrix.platform }}", "status": "passed", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'", "run_id": "${{ github.run_id }}", "sha": "${{ github.sha }}" }' > e2e-test-result.json
          cat e2e-test-result.json

      - name: Attest e2e test result
        uses: actions/attest-build-provenance@v3
        with:
          subject-path: e2e-test-result.json
          subject-name: e2e-test-result-${{ matrix.platform }}.json

      - name: Upload e2e test result
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-result-${{ matrix.platform }}
          path: e2e-test-result.json

      - name: Export Operator Logs on Failure
        if: failure()
        run: |
          echo "==== Frappe Operator Logs ===="
          kubectl logs -n frappe-operator-system -l control-plane=controller-manager --tail=1000
          echo "==== FrappeSite e2e-test-site status ===="
          kubectl get frappesite e2e-test-site -n default -o yaml || true
          echo "==== Init job e2e-test-site-init ===="
          kubectl describe job e2e-test-site-init -n default 2>/dev/null || true
          kubectl logs -n default job/e2e-test-site-init --tail=200 2>/dev/null || true
          
      - name: Cleanup
        if: always()
        run: |
          kubectl delete -f examples/kind-e2e-manifests.yaml --ignore-not-found
